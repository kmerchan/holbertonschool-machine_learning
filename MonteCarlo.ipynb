{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarlo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Xlqqhfb50kDUHlZ2cqzUX0ZmxZa9dR9q",
      "authorship_tag": "ABX9TyM8mPjzEvEygxSP2+x1JERs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmerchan/holbertonschool-machine_learning/blob/main/MonteCarlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TKw87CSJJy"
      },
      "source": [
        "%tensorflow_version 2.x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIAqbEhDJOjO",
        "outputId": "2912fe14-be81-4263-ed27-10674b639076"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_episode(env, policy, max_steps):\n",
        "    \"\"\"                                                                                                                 \n",
        "    Generates an episode using policy                                                                                   \n",
        "                                                                                                                        \n",
        "    parameters:                                                                                                         \n",
        "        env: the openAI environment instance                                                                            \n",
        "        policy: function that takes in state & returns the next action to take                                          \n",
        "        max_steps: the maximum number of steps per episode                                                              \n",
        "                                                                                                                        \n",
        "    returns:                                                                                                            \n",
        "        returns the episode                                                                                             \n",
        "    \"\"\"\n",
        "    episode = [[], []]\n",
        "    state = env.reset()\n",
        "    for step in range(max_steps):\n",
        "        action = policy(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode[0].append(state)\n",
        "\n",
        "        if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
        "            episode[1].append(-1)\n",
        "            return episode\n",
        "        if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
        "            episode[1].append(1)\n",
        "            return episode\n",
        "        episode[1].append(0)\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100,\n",
        "                alpha=0.1, gamma=0.99):\n",
        "    \"\"\"                                                                                                                 \n",
        "    Performs the Monte Carlo algorithm                                                                                  \n",
        "                                                                                                                        \n",
        "    parameters:                                                                                                         \n",
        "        env: the openAI environment instance                                                                            \n",
        "        V [numpy.ndarray of shape(s,)]: contains the value estimate                                                     \n",
        "        policy: function that takes in state & returns the next action to take                                          \n",
        "        episodes [int]: total number of episodes to train over                                                          \n",
        "        max_steps [int]: the maximum number of steps per episode                                                        \n",
        "        alpha [float]: the learning rate                                                                                \n",
        "        gamma [float]: the discount rate                                                                                \n",
        "                                                                                                                        \n",
        "    returns:                                                                                                            \n",
        "        V: the updated value estimate                                                                                   \n",
        "    \"\"\"\n",
        "    discounts = np.array([gamma ** i for i in range(max_steps)])\n",
        "    for ep in range(episodes):\n",
        "        episode = generate_episode(env, policy, max_steps)\n",
        "\n",
        "        for i in range(len(episode[0])):\n",
        "            Gt = np.sum(np.array(episode[1][i:]) *\n",
        "                        np.array(discounts[:len(episode[1][i:])]))\n",
        "            V[episode[0][i]] = (V[episode[0][i]] +\n",
        "                                alpha * (Gt - V[episode[0][i]]))\n",
        "    return V\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=2)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.79 -0.87 -0.84 -0.71 -0.61 -0.6  -0.34 -0.34]\n",
            " [-0.7  -0.71 -0.73 -0.43 -0.15 -0.48 -0.35 -0.6 ]\n",
            " [-0.91 -0.89 -0.94 -1.   -0.63 -0.3  -0.32 -0.63]\n",
            " [-0.92 -0.91 -0.95 -0.91 -0.79 -1.   -0.62 -0.48]\n",
            " [-0.89 -0.97 -0.96 -1.   -0.8  -0.91 -0.86 -0.45]\n",
            " [-0.96 -1.   -1.    0.62 -0.83 -0.79 -1.   -0.08]\n",
            " [-0.95 -1.   -0.74 -0.13 -1.   -0.5  -1.    0.28]\n",
            " [-0.93 -0.96 -0.94 -1.    1.    0.12  0.32  1.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}