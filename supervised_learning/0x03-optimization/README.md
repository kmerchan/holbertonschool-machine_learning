# 0x03-optimization
This directory contains work with noramlization, optimization, and hyperparameters:

## Mandatory Tasks:
0. [Normalization Constants](/supervised_learning/0x03-optimization/0-norm_constants.py)
* Write a function that calculates the normalization constants of a matrix.
1. [Normalize](/supervised_learning/0x03-optimization/1-normalize.py)
* Write a function that normalizes a matrix.
2. [Shuffle Data](/supervised_learning/0x03-optimization/2-shuffle_data.py)
* Write a function that shuffles the data points in two matrices the same way using numpy.random.permutation.
3. [Mini-Batch](/supervised_learning/0x03-optimization/3-mini_batch.py)
* Write a function that trains a loaded neural netwok model using mini-batch gradient descent.
4. [Moving Average](/supervised_learning/0x03-optimization/4-moving_average.py)
* Write a function that calculates the weighted moving average of a data set.
5. [Momentum](/supervised_learning/0x03-optimization/5-momentum.py)
* Write a function that updates a variable using the gradient descent with momentum optimization algorithm.
6. [Momentum Upgraded](/supervised_learning/0x03-optimization/6-momentum.py)
* Write a function that creates the training operation for a neural network in TensorFlow using the gradient descent with momentum optimization algorithm.
7. [RMSProp](/supervised_learning/0x03-optimization/7-RMSProp.py)
* Write a function that updates a variable using the RMSProp optimization algorithm.
8. [RMSProp Upgraded](/supervised_learning/0x03-optimization/8-RMSProp.py)
* Write a function that creates the training operation for a neural network in TensorFlow using the RMSProp optimization algorithm.
9. [Adam](/supervised_learning/0x03-optimization/9-Adam.py)
* Write a function that updates a variable in place using the Adam optimization algorithm.
10. [Adam Upgraded](/supervised_learning/0x03-optimization/10-Adam.py)
* Write a function that creates the training operation for a neural network in TensorFlow using the Adam optimization algorithm.
11. [Learning Rate Decay](/supervised_learning/0x03-optimization/11-learning_rate_decay.py)
* Write a function that updates the learning rate using inverse time decay in numpy.
12. [Learning Rate Decay Upgraded](/supervised_learning/0x03-optimization/12-learning_rate_decay.py)
* Write a funtion that creates a learning rate decay operation in TensorFlow using inverse time decay.
13. [Batch Normalization](/supervised_learning/0x03-optimization/13-batch_norm.py)
* Write a function that normalizes an unactivated output of a neural network using batch normalization.
14. [Batch Normalization Upgraded](/supervised_learning/0x03-optimization/14-batch_norm.py)
* Write a function that creates a batch normalization later for a neural network in TensorFlow.
15. [Put it all together and what do you get?](/supervised_learning/0x03-optimization/15-model.py)
* Write a function that builds, trains, adn saves a neural network model in TensorFlow using Adam optimization, mini-bath gradient descent, learning rate decay, and batch normalization.
16. [If you can't explain it simply, you don't understand it well enough]()
* Write a blog post explaining the mechanics, pros, and cons of the following oprimization techniques: feature scaling, batch normalization, mini-batch gradient descent, gradient descent with momentum, RMSProp optimization, Adam optimization, and learning rate decay.


### test_files directory
The test_files directory contains all files used to test output locally.

### data directory
The data directory contains datasets to train the code with.
