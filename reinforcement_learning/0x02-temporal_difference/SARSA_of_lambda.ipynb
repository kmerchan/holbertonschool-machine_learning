{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA of lambda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Xlqqhfb50kDUHlZ2cqzUX0ZmxZa9dR9q",
      "authorship_tag": "ABX9TyO92fp15xA/0Z/QdEvN0Vp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmerchan/holbertonschool-machine_learning/blob/main/SARSA_of_lambda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TKw87CSJJy"
      },
      "source": [
        "%tensorflow_version 2.x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIAqbEhDJOjO",
        "outputId": "fa253d11-d9a1-455b-ddeb-b8e192fe9ee8"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    \"\"\"                                                                                                                 \n",
        "    Uses epsilon-greedy to determine if the reinforcement learning is                                                   \n",
        "       exploring or exploiting and uses to get action                                                                   \n",
        "                                                                                                                        \n",
        "    parameters:                                                                                                         \n",
        "        Q [numpy.ndarray of shape (s, a)]: contains the Q table                                                         \n",
        "        state: the current state                                                                                        \n",
        "        epsilon: the threshold for epsilon-greedy                                                                       \n",
        "                                                                                                                        \n",
        "    returns:                                                                                                            \n",
        "        the action to take                                                                                              \n",
        "    \"\"\"\n",
        "    # determine exploring-exploiting balance by comparing to epsilon                                                    \n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        # exploring                                                                                                     \n",
        "        action = np.random.randint(Q.shape[1])\n",
        "    else:\n",
        "        # exploiting                                                                                                    \n",
        "        action = np.argmax(Q[state, :])\n",
        "    return action\n",
        "\n",
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
        "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    \"\"\"                                                                                                                 \n",
        "    Performs the SARSA(Î») algorithm                                                                                     \n",
        "                                                                                                                        \n",
        "    parameters:                                                                                                         \n",
        "        env: the openAI environment instance                                                                            \n",
        "        Q [numpy.ndarray of shape(s, a)]: contains the Q table                                                          \n",
        "        lambtha: the eligibility trace factor                                                                           \n",
        "        episodes [int]: total number of episodes to train over                                                          \n",
        "        max_steps [int]: the maximum number of steps per episode                                                        \n",
        "        alpha [float]: the learning rate                                                                                \n",
        "        gamma [float]: the discount rate                                                                                \n",
        "        epsilon: the initial threshold for epsilon greedy                                                               \n",
        "        min_epsilon [float]: the minimum value that epsilon should decay to                                             \n",
        "        epsilon_decay [float]: decay rate for updating epsilon between episodes                                         \n",
        "                                                                                                                        \n",
        "    returns:                                                                                                            \n",
        "        Q: the updated Q table                                                                                          \n",
        "    \"\"\"\n",
        "    # set maximum epsilon to the current epsilon before epsilon_decay                                                   \n",
        "    max_epsilon = epsilon\n",
        "    # Sets the eligibility traces to numpy array of zeros of same shape as Q                                            \n",
        "    Et = np.zeros((Q.shape))\n",
        "    # iterate over all episodes                                                                                         \n",
        "    for ep in range(episodes):\n",
        "        # set the initial state of each episode to environment reset                                                    \n",
        "        state = env.reset()\n",
        "        # get the action from epsilon-greedy function                                                                   \n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "        # iterate up to maximum number of steps per episode                                                             \n",
        "        for step in range(max_steps):\n",
        "            # eligibility traces updated with lambda & gamma                                                            \n",
        "            Et = Et * lambtha * gamma\n",
        "            # increase Et for current state, action                                                                     \n",
        "            Et[state, action] += 1\n",
        "\n",
        "            # perform the action to get next_state, reward, done, and info                                              \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # update the action, using epsilon-greedy again                                                             \n",
        "            next_action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            # if the algorithm finds a hole, the reward is updated to -1                                                \n",
        "            if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
        "                reward = -1\n",
        "            # if the algorithm finds the goal, the reward is updated to 1                                               \n",
        "            if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
        "                reward = 1\n",
        "\n",
        "            # calculate delta_t                                                                                         \n",
        "            # delta_t = R(t + 1) + gamma * Q(St + 1, At + 1) - Q(St, At)                                                \n",
        "            delta_t = reward + (\n",
        "                gamma * Q[next_state, next_action]) - Q[state, action]\n",
        "            # upddate Q table                                                                                           \n",
        "            # Q(st) = Q(st) + alpha * delta_t * Et(St)                                                                  \n",
        "            Q[state, action] = Q[state, action] + (\n",
        "                alpha * delta_t * Et[state, action])\n",
        "            # if done, break out of episode                                                                             \n",
        "            if done:\n",
        "                break\n",
        "            # otherwise, reset state, action and continue                                                               \n",
        "            state = next_state\n",
        "            action = next_action\n",
        "        # after each epsiode, update epsilon to decay                                                                   \n",
        "        # epsilon will now favor slightly more exploitation than exploration                                            \n",
        "        epsilon = min_epsilon + (\n",
        "            (max_epsilon - min_epsilon) * np.exp(-epsilon_decay * ep))\n",
        "    # when all episodes completed, return updated Q table                                                               \n",
        "    return Q\n",
        "\n",
        "np.random.seed(0)\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "Q = np.random.uniform(size=(64, 4))\n",
        "np.set_printoptions(precision=4)\n",
        "print(sarsa_lambtha(env, Q, 0.9))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.3879e-02  3.4557e-02  2.8270e-02  6.8923e-02]\n",
            " [ 3.0917e-02  3.4431e-02  3.9018e-02  9.7059e-02]\n",
            " [ 3.3157e-02  6.5353e-02  3.8297e-02  1.2059e-01]\n",
            " [ 8.2652e-02  6.8969e-02  2.7514e-02  1.4684e-01]\n",
            " [ 1.1357e-01  1.3502e-01  1.8097e-01  2.0670e-01]\n",
            " [ 1.9114e-01  2.1386e-01  2.8810e-01  2.7593e-01]\n",
            " [ 2.7993e-01  2.8333e-01  3.5798e-01  2.8041e-01]\n",
            " [ 2.9931e-01  3.4318e-01  3.4009e-01  3.0604e-01]\n",
            " [ 9.6911e-04 -8.1240e-03 -1.7234e-02  3.0431e-02]\n",
            " [-5.2000e-03 -2.7483e-02 -2.6210e-02  4.4929e-02]\n",
            " [-1.6855e-02 -8.6340e-02 -1.1419e-01  6.6851e-02]\n",
            " [-1.9790e-01 -2.9562e-01 -3.7827e-01  1.0026e-01]\n",
            " [-5.2111e-03 -1.0407e-01  2.6291e-02  1.2932e-01]\n",
            " [ 1.3394e-01  1.6509e-01  2.3974e-01  2.3793e-01]\n",
            " [ 2.5915e-01  2.5940e-01  3.8330e-01  3.0308e-01]\n",
            " [ 3.5390e-01  3.6438e-01  4.1513e-01  3.1072e-01]\n",
            " [-5.5764e-02 -6.2489e-02 -6.9592e-02 -5.7046e-02]\n",
            " [-7.6281e-02 -9.2457e-02 -8.5647e-02 -9.0307e-02]\n",
            " [-1.1340e-01 -2.5187e-01 -1.7623e-01 -2.2651e-01]\n",
            " [ 2.8281e-01  1.2020e-01  2.9614e-01  1.1873e-01]\n",
            " [-3.9790e-01 -1.7144e-01 -3.2394e-02 -2.1339e-01]\n",
            " [-1.3364e-01 -5.1889e-02  9.7087e-02  1.7599e-01]\n",
            " [ 1.9310e-01  2.2554e-01  3.4702e-01  2.6421e-01]\n",
            " [ 3.4567e-01  3.8964e-01  4.2653e-01  3.1731e-01]\n",
            " [-1.1029e-01 -1.2305e-01 -9.7398e-02 -8.7823e-02]\n",
            " [-1.2300e-01 -1.3051e-01 -1.5136e-01 -1.0410e-01]\n",
            " [-1.7058e-01 -8.6307e-02 -1.8222e-01 -1.3520e-01]\n",
            " [-4.4121e-01 -1.3934e-01 -2.6533e-01 -4.0385e-01]\n",
            " [ 4.3397e-01 -8.8755e-02 -1.4028e-01 -1.8060e-01]\n",
            " [ 8.8110e-01  5.8127e-01  8.8174e-01  6.9253e-01]\n",
            " [ 1.5224e-01 -7.0024e-02  3.1770e-01  1.5416e-01]\n",
            " [ 3.9164e-01  4.3395e-01  4.8366e-01  3.4656e-01]\n",
            " [-1.2207e-01 -2.1088e-01 -1.5538e-01 -2.1249e-01]\n",
            " [-3.3146e-01 -3.1851e-01 -4.0984e-01 -1.8443e-01]\n",
            " [ 2.0933e-01 -2.6633e-01 -6.4652e-03 -2.3855e-01]\n",
            " [ 8.9655e-01  3.6756e-01  4.3586e-01  8.9192e-01]\n",
            " [-3.1838e-03 -2.3981e-02  1.5570e-01  6.8259e-01]\n",
            " [ 4.2482e-01  7.9049e-01 -6.0616e-02 -8.7703e-02]\n",
            " [-6.7066e-02 -6.9499e-02  3.4567e-02  2.5712e-01]\n",
            " [ 4.3224e-01  3.9116e-01  7.1815e-01  3.2126e-01]\n",
            " [-1.8886e-01 -1.4494e-01 -1.8087e-01 -1.6693e-01]\n",
            " [ 9.7552e-01  8.5580e-01  1.1714e-02  3.5998e-01]\n",
            " [ 7.2999e-01  1.7163e-01  5.2104e-01  5.4338e-02]\n",
            " [ 6.2415e-02 -1.4966e-01  7.7467e-01  2.2392e-01]\n",
            " [ 3.4535e-01  7.4163e-01  3.2493e-01  6.7712e-02]\n",
            " [ 1.6469e-01  1.6883e-01  5.7723e-01  5.1434e-02]\n",
            " [ 9.3421e-01  6.1397e-01  5.3563e-01  5.8991e-01]\n",
            " [ 3.5698e-01  5.8079e-01  9.6132e-01  2.5819e-01]\n",
            " [-3.4595e-02 -4.2616e-01 -3.3062e-01 -2.3814e-01]\n",
            " [ 2.2741e-01  2.5436e-01  5.8029e-02  4.3442e-01]\n",
            " [-6.5921e-02  6.9634e-01  3.7775e-01  8.6825e-02]\n",
            " [ 2.4679e-02  6.7250e-02  4.8070e-01  4.5370e-01]\n",
            " [ 5.3658e-01  8.9667e-01  9.9034e-01  2.1690e-01]\n",
            " [ 5.6997e-01  2.6179e-01  2.3772e-02  7.5838e-01]\n",
            " [ 3.2002e-01  3.8346e-01  5.8832e-01  8.3105e-01]\n",
            " [ 9.1508e-01  9.6736e-01  1.3124e+00  5.7339e-01]\n",
            " [-8.4845e-02 -2.7368e-02 -8.2630e-02 -4.7080e-02]\n",
            " [-2.0852e-01 -6.9897e-02 -6.2349e-02 -1.4464e-01]\n",
            " [ 2.4067e-01 -1.5570e-01  6.7376e-02  2.9898e-01]\n",
            " [ 3.7417e-01  4.6358e-01  2.7763e-01  5.8678e-01]\n",
            " [ 4.9017e-01  1.1753e-01  5.1738e-01  1.3207e-01]\n",
            " [ 7.1686e-01  3.9606e-01  5.6542e-01  3.0902e-01]\n",
            " [ 6.2045e-02  4.8806e-01  3.5561e-01  9.4043e-01]\n",
            " [ 7.6533e-01  7.4866e-01  9.0372e-01  8.3422e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}