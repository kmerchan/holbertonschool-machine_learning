{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD of lambda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Xlqqhfb50kDUHlZ2cqzUX0ZmxZa9dR9q",
      "authorship_tag": "ABX9TyNtjYcBGy+Zob5cedrhcmrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmerchan/holbertonschool-machine_learning/blob/main/TD_of_lambda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TKw87CSJJy"
      },
      "source": [
        "%tensorflow_version 2.x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIAqbEhDJOjO",
        "outputId": "4c66b88b-7f7c-4165-dd83-ad6a596fa851"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100,\n",
        "               alpha=0.1, gamma=0.99):\n",
        "    \"\"\"                                                                                                                 \n",
        "    Performs the TD(Î») algorithm                                                                                        \n",
        "                                                                                                                        \n",
        "    parameters:                                                                                                         \n",
        "        env: the openAI environment instance                                                                            \n",
        "        V [numpy.ndarray of shape(s,)]: contains the value estimate                                                     \n",
        "        policy: function that takes in state & returns the next action to take                                          \n",
        "        episodes [int]: total number of episodes to train over                                                          \n",
        "        max_steps [int]: the maximum number of steps per episode                                                        \n",
        "        alpha [float]: the learning rate                                                                                \n",
        "        gamma [float]: the discount rate                                                                                \n",
        "                                                                                                                        \n",
        "    returns:                                                                                                            \n",
        "        V: the updated value estimate                                                                                   \n",
        "    \"\"\"\n",
        "    episode = [[], []]\n",
        "    Et = [0 for i in range(env.observation_space.n)]\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        for step in range(max_steps):\n",
        "            Et = list(np.array(Et) * lambtha * gamma)\n",
        "            Et[state] += 1\n",
        "\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
        "                reward = -1\n",
        "\n",
        "            if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
        "                reward = 1\n",
        "\n",
        "            delta_t = reward + gamma * V[next_state] - V[state]\n",
        "\n",
        "            V[state] = V[state] + alpha * delta_t * Et[state]\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "    return np.array(V)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.7024 -1.6418 -1.6322 -1.5696 -1.4827 -1.4578 -1.4339 -1.4317]\n",
            " [-1.7394 -1.7526 -1.7903 -1.8128 -1.6445 -1.5156 -1.4666 -1.4487]\n",
            " [-1.8045 -1.8105 -1.9037 -1.     -1.7291 -1.672  -1.5307 -1.4744]\n",
            " [-1.84   -1.8541 -1.9022 -1.9559 -1.8631 -1.     -1.676  -1.3938]\n",
            " [-1.8805 -1.927  -1.9208 -1.     -1.72   -1.5595 -1.4392 -0.9263]\n",
            " [-1.9187 -1.     -1.      0.9989 -1.7327 -1.5748 -1.     -0.8238]\n",
            " [-1.8715 -1.     -0.374  -0.2578 -1.     -1.2385 -1.     -0.0967]\n",
            " [-1.5725 -1.3142 -1.487  -1.      1.      0.3808  0.3299  1.    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}